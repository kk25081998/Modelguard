{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {
                "colab_type": "text",
                "id": "view-in-github"
            },
            "source": [
                "<a href=\"https://colab.research.google.com/github/kk25081998/Modelguard/blob/main/examples/1-Detect-Bad-Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "title"
            },
            "source": [
                "# üõ°Ô∏è ModelGuard Quick Start: Detect Bad Models\n",
                "\n",
                "Welcome to ModelGuard! This notebook demonstrates how to protect your ML applications from malicious model files.\n",
                "\n",
                "**What you'll learn:**\n",
                "- How malicious ML models can execute arbitrary code\n",
                "- How to detect and block dangerous models\n",
                "- Safe loading techniques for PyTorch, TensorFlow, and scikit-learn\n",
                "- Real-world security best practices\n",
                "\n",
                "**No setup required** - this runs entirely in Google Colab!"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "installation"
            },
            "source": [
                "## üì¶ Installation\n",
                "\n",
                "First, let's install ModelGuard and the ML frameworks we'll use for demonstration:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "install"
            },
            "outputs": [],
            "source": [
                "# Install ModelGuard and dependencies\n",
                "!pip install ml-modelguard torch scikit-learn --quiet\n",
                "\n",
                "print(\"‚úÖ Installation complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "problem"
            },
            "source": [
                "## üö® The Problem: Malicious Models\n",
                "\n",
                "ML models saved with Python's `pickle` format can contain arbitrary code that executes when loaded. Let's create a \"malicious\" model to demonstrate this risk:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "create_malicious"
            },
            "outputs": [],
            "source": [
                "import pickle\n",
                "import tempfile\n",
                "import os\n",
                "from pathlib import Path\n",
                "\n",
                "# Create a malicious model that executes code when loaded\n",
                "class MaliciousModel:\n",
                "    def __reduce__(self):\n",
                "        # This code will execute when the model is unpickled!\n",
                "        return (print, (\"üö® DANGER: Malicious code executed! This could have been anything...\",))\n",
                "\n",
                "# Save the malicious model\n",
                "malicious_path = \"/tmp/malicious_model.pkl\"\n",
                "with open(malicious_path, 'wb') as f:\n",
                "    pickle.dump(MaliciousModel(), f)\n",
                "\n",
                "print(f\"Created malicious model at: {malicious_path}\")\n",
                "print(\"‚ö†Ô∏è  This model will execute code when loaded with standard pickle.load()\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "demonstrate_danger"
            },
            "source": [
                "### Demonstrating the Danger\n",
                "\n",
                "Let's see what happens when we load this model with standard Python pickle:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "unsafe_load"
            },
            "outputs": [],
            "source": [
                "# WARNING: This demonstrates unsafe loading - DON'T do this with untrusted models!\n",
                "print(\"Loading malicious model with standard pickle...\")\n",
                "\n",
                "with open(malicious_path, 'rb') as f:\n",
                "    dangerous_model = pickle.load(f)  # Code executes here!\n",
                "\n",
                "print(\"\\nüíÄ As you can see, the malicious code executed automatically!\")\n",
                "print(\"   In a real attack, this could:\")\n",
                "print(\"   - Steal your data\")\n",
                "print(\"   - Install malware\")\n",
                "print(\"   - Access your cloud credentials\")\n",
                "print(\"   - Anything the attacker wants!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "solution"
            },
            "source": [
                "## ‚úÖ The Solution: ModelGuard\n",
                "\n",
                "Now let's see how ModelGuard protects you from these attacks:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "safe_scan"
            },
            "outputs": [],
            "source": [
                "from modelguard.core.scanner import ModelScanner\n",
                "from pathlib import Path\n",
                "\n",
                "# Scan the malicious model\n",
                "scanner = ModelScanner()\n",
                "result = scanner.scan_file(Path(malicious_path))\n",
                "\n",
                "print(\"üîç ModelGuard Scan Results:\")\n",
                "print(f\"   Is Safe: {result.is_safe}\")\n",
                "print(f\"   Threats Found: {len(result.threats)}\")\n",
                "print(f\"   Threat Details: {result.threats}\")\n",
                "\n",
                "if not result.is_safe:\n",
                "    print(\"\\nüõ°Ô∏è ModelGuard detected the malicious content and blocked it!\")\n",
                "else:\n",
                "    print(\"\\n‚ö†Ô∏è  Model appears safe (this shouldn't happen with our malicious example)\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "safe_loading"
            },
            "source": [
                "## üîí Safe Loading with ModelGuard\n",
                "\n",
                "Let's try to load the malicious model using ModelGuard's safe loading:\n",
                "\n",
                "**Note**: ModelGuard has different enforcement modes. By default, it only logs warnings. For this demo, we'll enable strict enforcement mode to actually block malicious models."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "safe_load_attempt"
            },
            "outputs": [],
            "source": [
                "from modelguard import sklearn\n",
                "import os\n",
                "\n",
                "print(\"Attempting to load malicious model with ModelGuard...\")\n",
                "\n",
                "# Enable enforcement mode for this demonstration\n",
                "os.environ[\"MODELGUARD_ENFORCE\"] = \"true\"\n",
                "os.environ[\"MODELGUARD_SCAN_ON_LOAD\"] = \"true\"\n",
                "\n",
                "try:\n",
                "    # This should fail safely\n",
                "    model = sklearn.safe_load(malicious_path)\n",
                "    print(\"‚ùå Unexpected: Model loaded (this shouldn't happen!)\")\n",
                "except Exception as e:\n",
                "    print(f\"‚úÖ ModelGuard blocked the malicious model: {type(e).__name__}\")\n",
                "    print(f\"   Error details: {str(e)[:100]}...\")\n",
                "    print(\"\\nüõ°Ô∏è Your system is protected!\")\n",
                "finally:\n",
                "    # Clean up environment variables\n",
                "    os.environ.pop(\"MODELGUARD_ENFORCE\", None)\n",
                "    os.environ.pop(\"MODELGUARD_SCAN_ON_LOAD\", None)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "legitimate_models"
            },
            "source": [
                "## ‚úÖ Working with Legitimate Models\n",
                "\n",
                "Now let's create and work with legitimate models to show that ModelGuard doesn't interfere with normal usage:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "create_safe_models"
            },
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "from sklearn.linear_model import LinearRegression\n",
                "import numpy as np\n",
                "\n",
                "# Create a legitimate PyTorch model\n",
                "class SimpleNet(nn.Module):\n",
                "    def __init__(self):\n",
                "        super().__init__()\n",
                "        self.linear = nn.Linear(10, 1)\n",
                "    \n",
                "    def forward(self, x):\n",
                "        return self.linear(x)\n",
                "\n",
                "pytorch_model = SimpleNet()\n",
                "pytorch_path = \"/tmp/safe_pytorch_model.pth\"\n",
                "torch.save(pytorch_model.state_dict(), pytorch_path)\n",
                "\n",
                "# Create a legitimate scikit-learn model\n",
                "X = np.random.randn(100, 5)\n",
                "y = np.random.randn(100)\n",
                "sklearn_model = LinearRegression().fit(X, y)\n",
                "sklearn_path = \"/tmp/safe_sklearn_model.pkl\"\n",
                "\n",
                "with open(sklearn_path, 'wb') as f:\n",
                "    pickle.dump(sklearn_model, f)\n",
                "\n",
                "print(\"‚úÖ Created legitimate models:\")\n",
                "print(f\"   PyTorch model: {pytorch_path}\")\n",
                "print(f\"   Scikit-learn model: {sklearn_path}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "scan_safe_models"
            },
            "source": [
                "### Scanning Legitimate Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "scan_legitimate"
            },
            "outputs": [],
            "source": [
                "# Scan the legitimate models\n",
                "print(\"üîç Scanning legitimate models...\\n\")\n",
                "\n",
                "for name, path in [(\"PyTorch\", pytorch_path), (\"Scikit-learn\", sklearn_path)]:\n",
                "    result = scanner.scan_file(Path(path))\n",
                "    print(f\"{name} Model:\")\n",
                "    print(f\"   ‚úÖ Is Safe: {result.is_safe}\")\n",
                "    print(f\"   üîç Threats: {len(result.threats)}\")\n",
                "    if result.threats:\n",
                "        print(f\"   ‚ö†Ô∏è  Details: {result.threats}\")\n",
                "    print()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "safe_loading_demo"
            },
            "source": [
                "### Safe Loading of Legitimate Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "load_legitimate"
            },
            "outputs": [],
            "source": [
                "from modelguard import torch as safe_torch\n",
                "from modelguard import sklearn as safe_sklearn\n",
                "\n",
                "print(\"üîí Loading legitimate models with ModelGuard...\\n\")\n",
                "\n",
                "# Load PyTorch model safely\n",
                "try:\n",
                "    loaded_pytorch = safe_torch.safe_load(pytorch_path)\n",
                "    print(\"‚úÖ PyTorch model loaded successfully!\")\n",
                "    print(f\"   Model keys: {list(loaded_pytorch.keys())[:3]}...\")\n",
                "except Exception as e:\n",
                "    print(f\"‚ùå PyTorch loading failed: {e}\")\n",
                "\n",
                "# Load scikit-learn model safely\n",
                "try:\n",
                "    loaded_sklearn = safe_sklearn.safe_load(sklearn_path)\n",
                "    print(\"\\n‚úÖ Scikit-learn model loaded successfully!\")\n",
                "    print(f\"   Model type: {type(loaded_sklearn).__name__}\")\n",
                "    print(f\"   Coefficients shape: {loaded_sklearn.coef_.shape}\")\n",
                "except Exception as e:\n",
                "    print(f\"‚ùå Scikit-learn loading failed: {e}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "context_manager"
            },
            "source": [
                "## üéØ Advanced: Context Manager (Recommended)\n",
                "\n",
                "The most convenient way to use ModelGuard is with the context manager, which automatically secures all model loading:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "context_demo"
            },
            "outputs": [],
            "source": [
                "import modelguard\n",
                "import torch\n",
                "import joblib\n",
                "\n",
                "print(\"üéØ Using ModelGuard context manager...\\n\")\n",
                "\n",
                "# Set enforcement policy for this demo\n",
                "import os\n",
                "os.environ[\"MODELGUARD_ENFORCE\"] = \"true\"\n",
                "os.environ[\"MODELGUARD_SCAN_ON_LOAD\"] = \"true\"\n",
                "\n",
                "# Everything inside this context is automatically secured\n",
                "with modelguard.patched():\n",
                "    print(\"1. Loading PyTorch model with torch.load()...\")\n",
                "    try:\n",
                "        model = torch.load(pytorch_path)  # Automatically secured!\n",
                "        print(\"   ‚úÖ Success! PyTorch model loaded safely\")\n",
                "    except Exception as e:\n",
                "        print(f\"   ‚ùå Failed: {e}\")\n",
                "    \n",
                "    print(\"\\n2. Loading scikit-learn model with joblib.load()...\")\n",
                "    try:\n",
                "        # Save model with joblib for this test\n",
                "        joblib_path = \"/tmp/sklearn_joblib_model.pkl\"\n",
                "        joblib.dump(sklearn_model, joblib_path)\n",
                "        \n",
                "        model = joblib.load(joblib_path)  # Automatically secured!\n",
                "        print(\"   ‚úÖ Success! Scikit-learn model loaded safely\")\n",
                "    except Exception as e:\n",
                "        print(f\"   ‚ùå Failed: {e}\")\n",
                "\n",
                "print(\"\\nüõ°Ô∏è Context manager provides seamless protection for framework loaders!\")\n",
                "print(\"üí° Note: For direct pickle.load(), use modelguard.sklearn.safe_load() instead\")\n",
                "\n",
                "# Clean up environment\n",
                "os.environ.pop(\"MODELGUARD_ENFORCE\", None)\n",
                "os.environ.pop(\"MODELGUARD_SCAN_ON_LOAD\", None)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "policy_config"
            },
            "source": [
                "## ‚öôÔ∏è Policy Configuration\n",
                "\n",
                "ModelGuard allows you to configure security policies for your organization:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "policy_demo"
            },
            "outputs": [],
            "source": [
                "from modelguard.core.policy import Policy, PolicyConfig\n",
                "import os\n",
                "\n",
                "print(\"‚öôÔ∏è Configuring ModelGuard policies...\\n\")\n",
                "\n",
                "# Method 1: Environment variables\n",
                "os.environ[\"MODELGUARD_ENFORCE\"] = \"true\"\n",
                "os.environ[\"MODELGUARD_SCAN_ON_LOAD\"] = \"true\"\n",
                "os.environ[\"MODELGUARD_MAX_FILE_SIZE_MB\"] = \"100\"\n",
                "\n",
                "print(\"Environment-based policy:\")\n",
                "print(f\"   MODELGUARD_ENFORCE: {os.environ.get('MODELGUARD_ENFORCE')}\")\n",
                "print(f\"   MODELGUARD_SCAN_ON_LOAD: {os.environ.get('MODELGUARD_SCAN_ON_LOAD')}\")\n",
                "print(f\"   MODELGUARD_MAX_FILE_SIZE_MB: {os.environ.get('MODELGUARD_MAX_FILE_SIZE_MB')}\")\n",
                "\n",
                "# Method 2: Programmatic configuration\n",
                "config = PolicyConfig(\n",
                "    enforce=True,\n",
                "    require_signatures=False,  # Set to False for this demo\n",
                "    scan_on_load=True,\n",
                "    max_file_size_mb=50\n",
                ")\n",
                "\n",
                "policy = Policy(config)\n",
                "\n",
                "print(\"\\nProgrammatic policy:\")\n",
                "print(f\"   Enforce mode: {policy.should_enforce()}\")\n",
                "print(f\"   Scan on load: {policy.should_scan()}\")\n",
                "print(f\"   Max file size: {policy.get_max_file_size()} bytes\")\n",
                "print(f\"   Requires signatures: {policy.requires_signatures()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "real_world"
            },
            "source": [
                "## üåç Real-World Usage Patterns\n",
                "\n",
                "Here are some common patterns for using ModelGuard in production:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "production_patterns"
            },
            "outputs": [],
            "source": [
                "print(\"üåç Real-world usage patterns:\\n\")\n",
                "\n",
                "# Pattern 1: Drop-in replacement\n",
                "print(\"1. Drop-in Replacement Pattern:\")\n",
                "print(\"   # Before: import torch\")\n",
                "print(\"   # After:  import modelguard.torch as torch\")\n",
                "print(\"   # All torch.load() calls are now secured!\")\n",
                "\n",
                "# Pattern 2: Explicit safe loading\n",
                "print(\"\\n2. Explicit Safe Loading Pattern:\")\n",
                "print(\"   model = modelguard.torch.safe_load('model.pth')\")\n",
                "print(\"   # Clear intent, explicit security\")\n",
                "\n",
                "# Pattern 3: Context manager for mixed loading\n",
                "print(\"\\n3. Context Manager Pattern:\")\n",
                "print(\"   with modelguard.patched():\")\n",
                "print(\"       # All model loading is secured\")\n",
                "print(\"       pytorch_model = torch.load('model.pth')\")\n",
                "print(\"       sklearn_model = pickle.load(open('model.pkl', 'rb'))\")\n",
                "\n",
                "# Pattern 4: Enterprise security\n",
                "print(\"\\n4. Enterprise Security Pattern:\")\n",
                "print(\"   # Set strict policies via environment\")\n",
                "print(\"   export MODELGUARD_ENFORCE=true\")\n",
                "print(\"   export MODELGUARD_REQUIRE_SIGNATURES=true\")\n",
                "print(\"   # All applications automatically secured\")\n",
                "\n",
                "print(\"\\n‚ú® Choose the pattern that fits your workflow!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "performance"
            },
            "source": [
                "## ‚ö° Performance Demonstration\n",
                "\n",
                "Let's measure ModelGuard's performance impact:"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "performance_test"
            },
            "outputs": [],
            "source": [
                "import time\n",
                "\n",
                "print(\"‚ö° Performance comparison...\\n\")\n",
                "\n",
                "# Create a larger model for testing\n",
                "large_model = {\n",
                "    'weights': np.random.randn(1000, 1000).tolist(),\n",
                "    'bias': np.random.randn(1000).tolist(),\n",
                "    'metadata': {'version': '1.0', 'framework': 'test'}\n",
                "}\n",
                "\n",
                "large_model_path = \"/tmp/large_model.pkl\"\n",
                "with open(large_model_path, 'wb') as f:\n",
                "    pickle.dump(large_model, f)\n",
                "\n",
                "file_size = os.path.getsize(large_model_path) / (1024 * 1024)  # MB\n",
                "print(f\"Test model size: {file_size:.1f} MB\")\n",
                "\n",
                "# Time scanning\n",
                "start_time = time.time()\n",
                "scan_result = scanner.scan_file(Path(large_model_path))\n",
                "scan_time = time.time() - start_time\n",
                "\n",
                "print(f\"\\nüîç Scanning performance:\")\n",
                "print(f\"   Time: {scan_time*1000:.1f} ms\")\n",
                "print(f\"   Rate: {file_size/scan_time:.1f} MB/s\")\n",
                "print(f\"   Result: {'‚úÖ Safe' if scan_result.is_safe else '‚ùå Unsafe'}\")\n",
                "\n",
                "# Time safe loading\n",
                "start_time = time.time()\n",
                "try:\n",
                "    loaded_model = sklearn.safe_load(large_model_path)\n",
                "    load_time = time.time() - start_time\n",
                "    print(f\"\\nüîí Safe loading performance:\")\n",
                "    print(f\"   Time: {load_time*1000:.1f} ms\")\n",
                "    print(f\"   Rate: {file_size/load_time:.1f} MB/s\")\n",
                "    print(f\"   Status: ‚úÖ Success\")\n",
                "except Exception as e:\n",
                "    print(f\"\\nüîí Safe loading: ‚ùå {e}\")\n",
                "\n",
                "print(f\"\\nüöÄ ModelGuard adds minimal overhead while providing comprehensive security!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "cleanup"
            },
            "source": [
                "## üßπ Cleanup"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {
                "id": "cleanup_files"
            },
            "outputs": [],
            "source": [
                "# Clean up temporary files\n",
                "for path in [malicious_path, pytorch_path, sklearn_path, large_model_path]:\n",
                "    if os.path.exists(path):\n",
                "        os.remove(path)\n",
                "\n",
                "print(\"üßπ Cleanup complete!\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {
                "id": "summary"
            },
            "source": [
                "## üéâ Summary\n",
                "\n",
                "Congratulations! You've learned how to protect your ML applications with ModelGuard:\n",
                "\n",
                "### ‚úÖ What We Covered\n",
                "- **The Risk**: ML models can contain malicious code that executes when loaded\n",
                "- **Detection**: ModelGuard scans models for dangerous patterns\n",
                "- **Protection**: Safe loading prevents malicious code execution\n",
                "- **Flexibility**: Multiple usage patterns for different workflows\n",
                "- **Performance**: Minimal overhead with comprehensive security\n",
                "\n",
                "### üöÄ Next Steps\n",
                "1. **Install ModelGuard** in your projects: `pip install ml-modelguard`\n",
                "2. **Choose your pattern**: Drop-in replacement, explicit safe loading, or context manager\n",
                "3. **Configure policies**: Set up organizational security policies\n",
                "4. **Stay secure**: Always scan models from untrusted sources\n",
                "\n",
                "### üìö Learn More\n",
                "- **GitHub**: https://github.com/kk25081998/Modelguard\n",
                "- **PyPI**: https://pypi.org/project/ml-modelguard/\n",
                "- **Documentation**: Check the README for advanced features\n",
                "\n",
                "### üõ°Ô∏è Remember\n",
                "**Never load untrusted models without ModelGuard protection!**\n",
                "\n",
                "---\n",
                "*Made with ‚ù§Ô∏è for the ML community's security*"
            ]
        }
    ],
    "metadata": {
        "colab": {
            "provenance": [],
            "toc_visible": true
        },
        "kernelspec": {
            "display_name": "Python 3",
            "name": "python3"
        },
        "language_info": {
            "name": "python"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 0
}